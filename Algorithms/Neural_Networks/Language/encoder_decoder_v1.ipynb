{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder Decoder Model\n",
        "## Sequence to Sequence Learning with Neural Networks (https://arxiv.org/abs/1409.3215)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IpoJjHRzCfgN"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "# Comments: https://colab.research.google.com/drive/1NmWujB2PoJk24uOwZ4cAfX3O8cZyigyf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq1.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4X1cgGDCC1SU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E7Da-Pm0C1ky"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkwI32QnIQlt",
        "outputId": "730feed4-f9ba-409f-ed7b-9a2f1f243aae"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "is_ignore_pads = True\n",
        "MAX_LENGTH = 10\n",
        "hidden_size = 128\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "SPLIT_RATIO = 0.95\n",
        "\n",
        "\n",
        "ENG_PREFIXES = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Data location\n",
        "file_path = 'data/eng-fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MJQCuQe_C4gD"
      },
      "outputs": [],
      "source": [
        "# Language class handler\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
        "        self.n_words = 3  # Count SOS, EOS and PAD_token\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "class PreProcess(object):\n",
        "  # Turn a Unicode string to plain ASCII, thanks to\n",
        "  # https://stackoverflow.com/a/518232/2809427\n",
        "  def unicodeToAscii(s):\n",
        "      return ''.join(\n",
        "          c for c in unicodedata.normalize('NFD', s)\n",
        "          if unicodedata.category(c) != 'Mn'\n",
        "      )\n",
        "\n",
        "  # Lowercase, trim, and remove non-letter characters\n",
        "  def normalizeString(s):\n",
        "      s = PreProcess.unicodeToAscii(s.lower().strip())\n",
        "      s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "      s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "      return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "muq8MZ6zGkUO"
      },
      "outputs": [],
      "source": [
        "class DataHandler(object):\n",
        "\n",
        "  # read langs and create lang objects, and pairs\n",
        "  def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(file_path, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[PreProcess.normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "  # filter pairs with length < max length + containing the eng_prefixes as mentioned in eng_prefixes\n",
        "  def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "        # and \\\n",
        "        # p[1].startswith(ENG_PREFIXES)\n",
        "\n",
        "  # filter pairs\n",
        "  def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if DataHandler.filterPair(pair)]\n",
        "\n",
        "  # Read data, filter data, register language objects\n",
        "  def prepareData(lang1, lang2, reverse=False):\n",
        "\n",
        "    # initiate language objects, and get pairs\n",
        "    input_lang, output_lang, pairs = DataHandler.readLangs(lang1, lang2, reverse)\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = DataHandler.filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "\n",
        "    # Register pairs with lang objects\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "89jC4beJDXJg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class DataLoaderHandler(object):\n",
        "\n",
        "  def sentenceFromIndices(lang, indices):\n",
        "      return ' '.join([lang.index2word[index] for index in indices])\n",
        "\n",
        "  # create a list of token-indices from a list of token\n",
        "  def indexesFromSentence(lang, sentence):\n",
        "      return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "  # create tensor from sentence\n",
        "  def tensorFromSentence(lang, sentence):\n",
        "      indexes = DataLoaderHandler.indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "      return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "  # create tensors from pair of sentences\n",
        "  def tensorsFromPair(pair):\n",
        "      input_tensor = DataLoaderHandler.tensorFromSentence(input_lang, pair[0])\n",
        "      target_tensor = DataLoaderHandler.tensorFromSentence(output_lang, pair[1])\n",
        "      return (input_tensor, target_tensor)\n",
        "\n",
        "  def split_train_test(pairs, split_ratio):\n",
        "\n",
        "    # Shuffle the data to ensure randomness\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    # Calculate the split indices\n",
        "    split_idx = int(len(pairs) * split_ratio)\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_pairs = pairs[:split_idx]\n",
        "    test_pairs = pairs[split_idx:]\n",
        "\n",
        "    # Optionally, if you want to further use the data as lists instead of references\n",
        "    train_pairs = list(train_pairs)\n",
        "    test_pairs = list(test_pairs)\n",
        "\n",
        "    return train_pairs, test_pairs\n",
        "\n",
        "  def tokenize_into_numpy_arrays(pairs, n, input_lang, output_lang):\n",
        "    # TODO: TRY INPUT AS VARIABLE LENGTH\n",
        "    # Init numpy arrays for timesteps with zeros. Should this be something else other than zeros to mark an empty token? (Since 0 is taken by SOS token)\n",
        "\n",
        "    input_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
        "    target_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
        "    # input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    # target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        # Get list of token-indices\n",
        "        inp_ids = DataLoaderHandler.indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = DataLoaderHandler.indexesFromSentence(output_lang, tgt)\n",
        "\n",
        "        # Append <end of string> tokens\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "\n",
        "        # Assign token indices in the main array\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "    return input_ids, target_ids\n",
        "\n",
        "  # generate data loader\n",
        "  def get_dataloader(batch_size):\n",
        "      # prepare language data\n",
        "      input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
        "\n",
        "      n = len(pairs)\n",
        "\n",
        "      train_pairs, test_pairs = DataLoaderHandler.split_train_test(pairs, SPLIT_RATIO)\n",
        "      n_train, n_test = len(train_pairs), len(test_pairs)\n",
        "\n",
        "      train_input_ids, train_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(train_pairs, n_train, input_lang, output_lang)\n",
        "      train_data = TensorDataset(\n",
        "                      torch.LongTensor(train_input_ids).to(device),\n",
        "                      torch.LongTensor(train_target_ids).to(device)\n",
        "      )\n",
        "\n",
        "      test_input_ids, test_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(test_pairs, n_test, input_lang, output_lang)\n",
        "      test_data = TensorDataset(\n",
        "                      torch.LongTensor(test_input_ids).to(device),\n",
        "                      torch.LongTensor(test_target_ids).to(device)\n",
        "      )\n",
        "\n",
        "      # Create a sampler\n",
        "      train_sampler = RandomSampler(train_data)\n",
        "      test_sampler = RandomSampler(test_data)\n",
        "\n",
        "      # Create a torch dataloader\n",
        "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "      test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=n_test)\n",
        "\n",
        "      print(f\"Train and Test Dataset # samples: {len(train_data)}, {len(test_data)}\")\n",
        "      print(f\"Train and Test Dataloader # batches: {len(train_dataloader)}, {len(test_dataloader)}\")\n",
        "\n",
        "      return input_lang, output_lang, train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJoyZiRuojNA",
        "outputId": "a4f35fc2-bc8c-4af8-8f80-2d4790100ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17865\n",
            "eng 10699\n",
            "Train and Test Dataset # samples: 100407, 5285\n",
            "Train and Test Dataloader # batches: 3138, 1\n"
          ]
        }
      ],
      "source": [
        "# Prepare Data\n",
        "# input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
        "# print(random.choice(pairs))\n",
        "\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCOFSxKeDkEB"
      },
      "source": [
        "**Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNGIszhqDjle"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "class Helpers(object):\n",
        "\n",
        "  def asMinutes(s):\n",
        "      m = math.floor(s / 60)\n",
        "      s -= m * 60\n",
        "      return '%dm %ds' % (m, s)\n",
        "\n",
        "  def timeSince(since, percent):\n",
        "      now = time.time()\n",
        "      s = now - since\n",
        "      es = s / (percent)\n",
        "      rs = es - s\n",
        "      return '%s (- %s)' % (Helpers.asMinutes(s), Helpers.asMinutes(rs))\n",
        "\n",
        "  def showPlot(points):\n",
        "      plt.figure()\n",
        "      fig, ax = plt.subplots()\n",
        "      # this locator puts ticks at regular intervals\n",
        "      loc = ticker.MultipleLocator(base=0.2)\n",
        "      ax.yaxis.set_major_locator(loc)\n",
        "      plt.plot(points)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViMMrjBDnlZ"
      },
      "source": [
        "The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQZI4vkXDJ2E"
      },
      "outputs": [],
      "source": [
        "# Comments: https://colab.research.google.com/drive/1NmWujB2PoJk24uOwZ4cAfX3O8cZyigyf#scrollTo=ARbOsC8bpH7O\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Convert to embedding {vocab_size, embedding_dimension: hidden_size}\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Embedding vector\n",
        "        embedding_vector = self.embedding(input)\n",
        "        embedding_vector = self.dropout(embedding_vector)\n",
        "\n",
        "        output, hidden = self.gru(embedding_vector)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        predicted_decoder_tokens = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # print(\"Without teacher forcing\")\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "                predicted_decoder_tokens.append(decoder_input)\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, predicted_decoder_tokens # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "class EncoderDecoderTranslation(nn.Module):\n",
        "\n",
        "  def __init__(self, input_lang, output_lang, hidden_size, device):\n",
        "        super(EncoderDecoderTranslation, self).__init__()\n",
        "\n",
        "        self.encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "        self.decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "        self.device = device\n",
        "\n",
        "  def forward(self, input_tensor, target_tensor=None):\n",
        "\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
        "    decoder_outputs, _, predicted_decoder_tokens = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "    return decoder_outputs, predicted_decoder_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0riCRmL9v7Il"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder_decoder, encoder_decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in tqdm(dataloader):\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # zero out gradients before each batch\n",
        "        encoder_decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Run encoder-decoder forward()\n",
        "        decoder_outputs, _ = encoder_decoder(input_tensor, target_tensor)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "\n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "        # update weights\n",
        "        encoder_decoder_optimizer.step()\n",
        "\n",
        "        # update epoch level loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def normalize_tensors_to_tokens(tensor, remove_first_idx=False):\n",
        "\n",
        "  # convert_to_list\n",
        "  tensor = tensor.tolist()\n",
        "\n",
        "  # remove_sos_eos_and_pads_convert_list\n",
        "  if remove_first_idx:\n",
        "    tensor = [sequence[1:] for sequence in tensor]\n",
        "\n",
        "  # remove all tokens after <eos token>\n",
        "  out_list = []\n",
        "  for sequence in tensor:\n",
        "    new_seq = []\n",
        "    for token in sequence:\n",
        "      if token == EOS_token:\n",
        "        break\n",
        "      new_seq.append(token)\n",
        "    out_list.append(new_seq)\n",
        "\n",
        "  return out_list\n",
        "\n",
        "\n",
        "def predict(data_loader, encoder_decoder):\n",
        "\n",
        "  # Eval Mode. Turn off dropout and batchnorm\n",
        "  encoder_decoder.eval()\n",
        "\n",
        "  list_decoder_outputs = []\n",
        "\n",
        "  # ensure no gradients are calculated with no_grad() to preserve memory\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      input_tensor, target_tensor = data\n",
        "      decoder_outputs, predicted_decoder_tokens = encoder_decoder(input_tensor)\n",
        "      list_decoder_outputs.append(decoder_outputs)\n",
        "\n",
        "      # Merge timesteps of decoder predictions\n",
        "      predicted_decoder_tokens = torch.cat(predicted_decoder_tokens, dim=1)\n",
        "\n",
        "  return list_decoder_outputs, input_tensor, target_tensor, predicted_decoder_tokens\n",
        "\n",
        "\n",
        "def calculate_bleu(test_target_tokens, predicted_decoder_tokens):\n",
        "\n",
        "  return corpus_bleu(\n",
        "      [[item] for item in test_target_tokens],\n",
        "      [item for item in predicted_decoder_tokens],\n",
        "    )\n",
        "\n",
        "def train(train_dataloader, test_dataloader, encoder_decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_decoder_optimizer = optim.Adam(encoder_decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Ignore pad token from loss calculation\n",
        "    if is_ignore_pads:\n",
        "      criterion = nn.NLLLoss(ignore_index = PAD_token)\n",
        "    else:\n",
        "      criterion = nn.NLLLoss()\n",
        "\n",
        "    print('Time \\t\\t\\t (Epoch\\t%) \\t Loss \\t\\t Bleu')\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Training\n",
        "        encoder_decoder.train()\n",
        "        loss = train_epoch(train_dataloader, encoder_decoder, encoder_decoder_optimizer, criterion)\n",
        "\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        # Eval on Test\n",
        "        encoder_decoder.eval()\n",
        "        # Evaluate without teacher forcing on test set\n",
        "        test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)\n",
        "\n",
        "        # Calculate bleu\n",
        "        bleu = calculate_bleu(\n",
        "            normalize_tensors_to_tokens(test_target_tensor, False),\n",
        "            normalize_tensors_to_tokens(predicted_decoder_tokens, False)\n",
        "        )\n",
        "\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s \\t (%d \\t %d%%) \\t %.4f \\t %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg, bleu))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    Helpers.showPlot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4NY1Q_Kq77W",
        "outputId": "9d320501-fd51-4b59-db8e-5191196a46ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17865\n",
            "eng 10699\n",
            "Train and Test Dataset # samples: 100407, 5285\n",
            "Train and Test Dataloader # batches: 1569, 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThKShpL_Z13a"
      },
      "outputs": [],
      "source": [
        "# is_ignore_pads = False\n",
        "# # init encoder-decoder\n",
        "# encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "# train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdJc9haR2TJE",
        "outputId": "e96d99b1-6ceb-49c1-db57-a000c1ee20f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17865\n",
            "eng 10699\n",
            "Train and Test Dataset # samples: 100407, 5285\n",
            "Train and Test Dataloader # batches: 1569, 1\n",
            "Time \t\t\t (Epoch\t%) \t Loss \t\t Bleu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 75.22it/s]\n",
            "100%|██████████| 1569/1569 [00:21<00:00, 72.57it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.21it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.10it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 46s (- 69m 30s) \t (5 \t 2%) \t 2.6795 \t 0.2050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.05it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.64it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.76it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.78it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3m 30s (- 66m 39s) \t (10 \t 5%) \t 1.4939 \t 0.2824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 75.51it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.97it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.06it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.94it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5m 14s (- 64m 41s) \t (15 \t 7%) \t 1.0873 \t 0.3209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 78.05it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.80it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.32it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.74it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6m 58s (- 62m 47s) \t (20 \t 10%) \t 0.8675 \t 0.3474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.45it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.63it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.98it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.95it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8m 41s (- 60m 52s) \t (25 \t 12%) \t 0.7288 \t 0.3668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:21<00:00, 72.09it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.12it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.07it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.50it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10m 27s (- 59m 14s) \t (30 \t 15%) \t 0.6329 \t 0.3685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.37it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.14it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.60it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.23it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12m 11s (- 57m 27s) \t (35 \t 17%) \t 0.5625 \t 0.3880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 78.81it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.14it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.94it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.35it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13m 54s (- 55m 39s) \t (40 \t 20%) \t 0.5088 \t 0.3906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.00it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.35it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.83it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.65it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15m 38s (- 53m 52s) \t (45 \t 22%) \t 0.4665 \t 0.3943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.00it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.56it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.72it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.85it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17m 22s (- 52m 8s) \t (50 \t 25%) \t 0.4315 \t 0.4010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.30it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.28it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.29it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.31it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19m 6s (- 50m 21s) \t (55 \t 27%) \t 0.4024 \t 0.3943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.23it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.64it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.07it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.72it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20m 49s (- 48m 35s) \t (60 \t 30%) \t 0.3782 \t 0.4057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.40it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.37it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.29it/s]\n",
            "100%|██████████| 1569/1569 [00:21<00:00, 73.88it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22m 33s (- 46m 52s) \t (65 \t 32%) \t 0.3577 \t 0.4125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.61it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.98it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.59it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.31it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24m 17s (- 45m 6s) \t (70 \t 35%) \t 0.3395 \t 0.4111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.17it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.88it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.49it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.92it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26m 0s (- 43m 21s) \t (75 \t 37%) \t 0.3240 \t 0.4136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.77it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.30it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.68it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.40it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27m 44s (- 41m 37s) \t (80 \t 40%) \t 0.3100 \t 0.4134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.65it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.12it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.20it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.07it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29m 28s (- 39m 52s) \t (85 \t 42%) \t 0.2973 \t 0.4129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.20it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.78it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.98it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.60it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31m 11s (- 38m 7s) \t (90 \t 45%) \t 0.2869 \t 0.4211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.62it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.65it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.50it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.21it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32m 55s (- 36m 22s) \t (95 \t 47%) \t 0.2768 \t 0.4210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.09it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.65it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.25it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.43it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34m 38s (- 34m 38s) \t (100 \t 50%) \t 0.2679 \t 0.4218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:21<00:00, 74.42it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.46it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.95it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.26it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36m 24s (- 32m 56s) \t (105 \t 52%) \t 0.2596 \t 0.4225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.63it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.56it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.30it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.64it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38m 8s (- 31m 12s) \t (110 \t 55%) \t 0.2521 \t 0.4221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.74it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.21it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.36it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.01it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39m 51s (- 29m 27s) \t (115 \t 57%) \t 0.2453 \t 0.4208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.51it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.04it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.95it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.26it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41m 35s (- 27m 43s) \t (120 \t 60%) \t 0.2396 \t 0.4242\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.42it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.18it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.65it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.94it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43m 18s (- 25m 59s) \t (125 \t 62%) \t 0.2333 \t 0.4334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.43it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.65it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.04it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.08it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45m 1s (- 24m 14s) \t (130 \t 65%) \t 0.2279 \t 0.4234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.39it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.01it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.03it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.03it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46m 45s (- 22m 30s) \t (135 \t 67%) \t 0.2232 \t 0.4246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.53it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.68it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.83it/s]\n",
            "100%|██████████| 1569/1569 [00:21<00:00, 74.59it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48m 31s (- 20m 47s) \t (140 \t 70%) \t 0.2183 \t 0.4313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.53it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.27it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.86it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.56it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50m 15s (- 19m 3s) \t (145 \t 72%) \t 0.2144 \t 0.4295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.84it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.91it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.20it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.20it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51m 58s (- 17m 19s) \t (150 \t 75%) \t 0.2101 \t 0.4303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 78.18it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.20it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.54it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.94it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53m 42s (- 15m 35s) \t (155 \t 77%) \t 0.2069 \t 0.4237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.37it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.10it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.93it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.31it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55m 25s (- 13m 51s) \t (160 \t 80%) \t 0.2025 \t 0.4304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.76it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.50it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 74.72it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.43it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57m 10s (- 12m 7s) \t (165 \t 82%) \t 0.1996 \t 0.4317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 78.99it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.26it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.94it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.46it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58m 54s (- 10m 23s) \t (170 \t 85%) \t 0.1963 \t 0.4284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.69it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.28it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.51it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.66it/s]\n",
            "100%|██████████| 1569/1569 [00:21<00:00, 74.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60m 39s (- 8m 39s) \t (175 \t 87%) \t 0.1931 \t 0.4285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.26it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.26it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.13it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.19it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62m 23s (- 6m 55s) \t (180 \t 90%) \t 0.1902 \t 0.4342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.95it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 75.73it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.90it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.22it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64m 7s (- 5m 11s) \t (185 \t 92%) \t 0.1875 \t 0.4331\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 77.40it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.11it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.23it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.54it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65m 50s (- 3m 27s) \t (190 \t 95%) \t 0.1850 \t 0.4299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 80.35it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.63it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.86it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.17it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67m 34s (- 1m 43s) \t (195 \t 97%) \t 0.1826 \t 0.4304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 78.53it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.32it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.66it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.31it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "69m 17s (- 0m 0s) \t (200 \t 100%) \t 0.1810 \t 0.4329\n"
          ]
        }
      ],
      "source": [
        "is_ignore_pads = True\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "\n",
        "# init encoder-decoder\n",
        "encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeU21-Ze83bB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def show_model_layers_and_params(model):\n",
        "    print(\"Model Layers:\")\n",
        "    print(\"--------------\")\n",
        "    for name, module in model.named_children():\n",
        "        print(f\"{name}: {module}\")\n",
        "\n",
        "    print(\"\\nLayer-wise Number of Parameters and Memory Requirements:\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    total_params = 0\n",
        "    total_memory = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            param_memory = num_params * param.element_size() / (1024 ** 2)  # Memory in MBs\n",
        "            print(f\"{name}: {num_params} parameters, {param_memory:.2f} MB\")\n",
        "            total_params += num_params\n",
        "            total_memory += param_memory\n",
        "\n",
        "    print(\"\\nTotal Number of Parameters and Memory Usage:\")\n",
        "    print(\"------------------------------------------\")\n",
        "    print(f\"Total parameters: {total_params}\")\n",
        "    print(f\"Total memory usage: {total_memory:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF3AUb9mmgW2"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MjYBvJQUckH",
        "outputId": "2aaa5676-c578-4359-94c1-6491b75f0758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "je voulais juste aller a l universite <EOS> <PAD> <PAD>\n",
            "i just wanted to go to college <EOS> <PAD> <PAD>\n",
            "i just wanted to go to college <EOS> <EOS> <EOS>\n",
            "\n",
            "j ai un nouveau velo <EOS> <PAD> <PAD> <PAD> <PAD>\n",
            "i ve got a new bike <EOS> <PAD> <PAD> <PAD>\n",
            "i have a new bicycle <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "\n",
            "je n aime pas cela <EOS> <PAD> <PAD> <PAD> <PAD>\n",
            "i don t like this <EOS> <PAD> <PAD> <PAD> <PAD>\n",
            "i don t like that <EOS> <EOS> <EOS> <EOS> <EOS>\n"
          ]
        }
      ],
      "source": [
        "# Example Predictions without normalizing tokens\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0].tolist()))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1].tolist()))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llYgWdxhRQHW"
      },
      "outputs": [],
      "source": [
        "# Normalize tokens\n",
        "test_input_tensor = normalize_tensors_to_tokens(test_input_tensor, False)\n",
        "test_target_tensor = normalize_tensors_to_tokens(test_target_tensor, False)\n",
        "predicted_decoder_tokens = normalize_tensors_to_tokens(predicted_decoder_tokens, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njMlc4YempAE",
        "outputId": "2ad3af29-43fb-4d2b-98f7-2833283cf07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "je voulais juste aller a l universite\n",
            "i just wanted to go to college\n",
            "just wanted to go to college\n",
            "\n",
            "j ai un nouveau velo\n",
            "i ve got a new bike\n",
            "have a new bicycle\n",
            "\n",
            "je n aime pas cela\n",
            "i don t like this\n",
            "don t like that\n"
          ]
        }
      ],
      "source": [
        "# Example Predictions with normalizing tokens\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0]))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1]))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "5LvNSuFpLcZ5",
        "outputId": "a25b612b-9178-4c66-e01b-2aa2e5e6a078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17865\n",
            "eng 10699\n",
            "Train and Test Dataset # samples: 100407, 5285\n",
            "Train and Test Dataloader # batches: 1569, 1\n",
            "Time \t\t\t (Epoch\t%) \t Loss \t\t Bleu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 79.16it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.63it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 80.26it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.16it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1m 43s (- 67m 6s) \t (5 \t 2%) \t 1.7946 \t 0.2093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:20<00:00, 76.47it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 79.98it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 76.76it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.92it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3m 27s (- 65m 43s) \t (10 \t 5%) \t 0.9899 \t 0.3008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:21<00:00, 73.38it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.23it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 78.24it/s]\n",
            "100%|██████████| 1569/1569 [00:19<00:00, 78.77it/s]\n",
            "100%|██████████| 1569/1569 [00:20<00:00, 77.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5m 13s (- 64m 25s) \t (15 \t 7%) \t 0.7173 \t 0.3431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1569/1569 [00:19<00:00, 78.82it/s]\n",
            " 15%|█▌        | 243/1569 [00:03<00:16, 79.13it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ef1ce036a715>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-ff914a982075>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, test_dataloader, encoder_decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_decoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ff914a982075>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder_decoder, encoder_decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mencoder_decoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "is_ignore_pads = False\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "\n",
        "# init encoder-decoder\n",
        "encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMGHE5HRqWgz"
      },
      "source": [
        "# TO TRY\n",
        "- Ignore padding token loss (ignore index) - mixed results\n",
        "- evaluation metric - bleu (done)\n",
        "- EOS token related sequence clipping - done\n",
        "- shifted target sequence? - ignore (done)\n",
        "- loss: what all to include? to include <EOS>? - eos included, padding not included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS7KdW3zDTQO"
      },
      "outputs": [],
      "source": [
        "# def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "#           decoder_optimizer, criterion):\n",
        "\n",
        "#     total_loss = 0\n",
        "#     for data in dataloader:\n",
        "#         input_tensor, target_tensor = data\n",
        "\n",
        "#         encoder_optimizer.zero_grad()\n",
        "#         decoder_optimizer.zero_grad()\n",
        "\n",
        "#         encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "#         decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "#         loss = criterion(\n",
        "#             decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "#             target_tensor.view(-1)\n",
        "#         )\n",
        "#         loss.backward()\n",
        "\n",
        "#         encoder_optimizer.step()\n",
        "#         decoder_optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)\n",
        "\n",
        "# def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "#                print_every=100, plot_every=100):\n",
        "#     start = time.time()\n",
        "#     plot_losses = []\n",
        "#     print_loss_total = 0  # Reset every print_every\n",
        "#     plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "#     encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "#     decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "#     criterion = nn.NLLLoss()\n",
        "\n",
        "#     for epoch in range(1, n_epochs + 1):\n",
        "#         loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "#         print_loss_total += loss\n",
        "#         plot_loss_total += loss\n",
        "\n",
        "#         if epoch % print_every == 0:\n",
        "#             print_loss_avg = print_loss_total / print_every\n",
        "#             print_loss_total = 0\n",
        "#             print('%s (%d %d%%) %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
        "#                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "#         if epoch % plot_every == 0:\n",
        "#             plot_loss_avg = plot_loss_total / plot_every\n",
        "#             plot_losses.append(plot_loss_avg)\n",
        "#             plot_loss_total = 0\n",
        "\n",
        "#     Helpers.showPlot(plot_losses)\n",
        "\n",
        "# hidden_size = 128\n",
        "# batch_size = 32\n",
        "# input_lang, output_lang, train_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "# encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# # train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QpTo7s10fgp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
