{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IpoJjHRzCfgN"
      },
      "outputs": [],
      "source": [
        "# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\n",
        "# Comments: https://colab.research.google.com/drive/1NmWujB2PoJk24uOwZ4cAfX3O8cZyigyf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4X1cgGDCC1SU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E7Da-Pm0C1ky"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkwI32QnIQlt",
        "outputId": "a3f26124-dbb2-4c5b-dcd1-8442b9a73e98"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "is_ignore_pads = True\n",
        "MAX_LENGTH = 10\n",
        "hidden_size = 128\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "SPLIT_RATIO = 0.95\n",
        "\n",
        "\n",
        "ENG_PREFIXES = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "# Data location\n",
        "file_path = 'data/eng-fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJQCuQe_C4gD"
      },
      "outputs": [],
      "source": [
        "# Language class handler\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
        "        self.n_words = 3  # Count SOS, EOS and PAD_token\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "class PreProcess(object):\n",
        "  # Turn a Unicode string to plain ASCII, thanks to\n",
        "  # https://stackoverflow.com/a/518232/2809427\n",
        "  def unicodeToAscii(s):\n",
        "      return ''.join(\n",
        "          c for c in unicodedata.normalize('NFD', s)\n",
        "          if unicodedata.category(c) != 'Mn'\n",
        "      )\n",
        "\n",
        "  # Lowercase, trim, and remove non-letter characters\n",
        "  def normalizeString(s):\n",
        "      s = PreProcess.unicodeToAscii(s.lower().strip())\n",
        "      s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "      s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "      return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "muq8MZ6zGkUO"
      },
      "outputs": [],
      "source": [
        "class DataHandler(object):\n",
        "\n",
        "  # read langs and create lang objects, and pairs\n",
        "  def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(file_path, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[PreProcess.normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "  # filter pairs with length < max length + containing the eng_prefixes as mentioned in eng_prefixes\n",
        "  def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "        # and \\\n",
        "        # p[1].startswith(ENG_PREFIXES)\n",
        "\n",
        "  # filter pairs\n",
        "  def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if DataHandler.filterPair(pair)]\n",
        "\n",
        "  # Read data, filter data, register language objects\n",
        "  def prepareData(lang1, lang2, reverse=False):\n",
        "\n",
        "    # initiate language objects, and get pairs\n",
        "    input_lang, output_lang, pairs = DataHandler.readLangs(lang1, lang2, reverse)\n",
        "\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = DataHandler.filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "\n",
        "    # Register pairs with lang objects\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "89jC4beJDXJg"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class DataLoaderHandler(object):\n",
        "\n",
        "  def sentenceFromIndices(lang, indices):\n",
        "      return ' '.join([lang.index2word[index] for index in indices])\n",
        "\n",
        "  # create a list of token-indices from a list of token\n",
        "  def indexesFromSentence(lang, sentence):\n",
        "      return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "  # create tensor from sentence\n",
        "  def tensorFromSentence(lang, sentence):\n",
        "      indexes = DataLoaderHandler.indexesFromSentence(lang, sentence)\n",
        "      indexes.append(EOS_token)\n",
        "      return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "  # create tensors from pair of sentences\n",
        "  def tensorsFromPair(pair):\n",
        "      input_tensor = DataLoaderHandler.tensorFromSentence(input_lang, pair[0])\n",
        "      target_tensor = DataLoaderHandler.tensorFromSentence(output_lang, pair[1])\n",
        "      return (input_tensor, target_tensor)\n",
        "\n",
        "  def split_train_test(pairs, split_ratio):\n",
        "\n",
        "    # Shuffle the data to ensure randomness\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    # Calculate the split indices\n",
        "    split_idx = int(len(pairs) * split_ratio)\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_pairs = pairs[:split_idx]\n",
        "    test_pairs = pairs[split_idx:]\n",
        "\n",
        "    # Optionally, if you want to further use the data as lists instead of references\n",
        "    train_pairs = list(train_pairs)\n",
        "    test_pairs = list(test_pairs)\n",
        "\n",
        "    return train_pairs, test_pairs\n",
        "\n",
        "  def tokenize_into_numpy_arrays(pairs, n, input_lang, output_lang):\n",
        "    # TODO: TRY INPUT AS VARIABLE LENGTH\n",
        "    # Init numpy arrays for timesteps with zeros. Should this be something else other than zeros to mark an empty token? (Since 0 is taken by SOS token)\n",
        "\n",
        "    input_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
        "    target_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
        "    # input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    # target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        # Get list of token-indices\n",
        "        inp_ids = DataLoaderHandler.indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = DataLoaderHandler.indexesFromSentence(output_lang, tgt)\n",
        "\n",
        "        # Append <end of string> tokens\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "\n",
        "        # Assign token indices in the main array\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "    return input_ids, target_ids\n",
        "\n",
        "  # generate data loader\n",
        "  def get_dataloader(batch_size):\n",
        "      # prepare language data\n",
        "      input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
        "\n",
        "      n = len(pairs)\n",
        "\n",
        "      train_pairs, test_pairs = DataLoaderHandler.split_train_test(pairs, SPLIT_RATIO)\n",
        "      n_train, n_test = len(train_pairs), len(test_pairs)\n",
        "\n",
        "      train_input_ids, train_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(train_pairs, n_train, input_lang, output_lang)\n",
        "      train_data = TensorDataset(\n",
        "                      torch.LongTensor(train_input_ids).to(device),\n",
        "                      torch.LongTensor(train_target_ids).to(device)\n",
        "      )\n",
        "\n",
        "      test_input_ids, test_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(test_pairs, n_test, input_lang, output_lang)\n",
        "      test_data = TensorDataset(\n",
        "                      torch.LongTensor(test_input_ids).to(device),\n",
        "                      torch.LongTensor(test_target_ids).to(device)\n",
        "      )\n",
        "\n",
        "      # Create a sampler\n",
        "      train_sampler = RandomSampler(train_data)\n",
        "      test_sampler = RandomSampler(test_data)\n",
        "\n",
        "      # Create a torch dataloader\n",
        "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "      test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=n_test)\n",
        "\n",
        "      print(f\"Train and Test Dataset # samples: {len(train_data)}, {len(test_data)}\")\n",
        "      print(f\"Train and Test Dataloader # batches: {len(train_dataloader)}, {len(test_dataloader)}\")\n",
        "\n",
        "      return input_lang, output_lang, train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJoyZiRuojNA",
        "outputId": "5211835b-9d3d-4d79-e52e-c8164a30c3df"
      },
      "outputs": [],
      "source": [
        "# Prepare Data\n",
        "# input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
        "# print(random.choice(pairs))\n",
        "\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCOFSxKeDkEB"
      },
      "source": [
        "**Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UNGIszhqDjle"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "class Helpers(object):\n",
        "\n",
        "  def asMinutes(s):\n",
        "      m = math.floor(s / 60)\n",
        "      s -= m * 60\n",
        "      return '%dm %ds' % (m, s)\n",
        "\n",
        "  def timeSince(since, percent):\n",
        "      now = time.time()\n",
        "      s = now - since\n",
        "      es = s / (percent)\n",
        "      rs = es - s\n",
        "      return '%s (- %s)' % (Helpers.asMinutes(s), Helpers.asMinutes(rs))\n",
        "\n",
        "  def showPlot(points):\n",
        "      plt.figure()\n",
        "      fig, ax = plt.subplots()\n",
        "      # this locator puts ticks at regular intervals\n",
        "      loc = ticker.MultipleLocator(base=0.2)\n",
        "      ax.yaxis.set_major_locator(loc)\n",
        "      plt.plot(points)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qViMMrjBDnlZ"
      },
      "source": [
        "The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wQZI4vkXDJ2E"
      },
      "outputs": [],
      "source": [
        "# Comments: https://colab.research.google.com/drive/1NmWujB2PoJk24uOwZ4cAfX3O8cZyigyf#scrollTo=ARbOsC8bpH7O\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Convert to embedding {vocab_size, embedding_dimension: hidden_size}\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Embedding vector\n",
        "        embedding_vector = self.embedding(input)\n",
        "        embedding_vector = self.dropout(embedding_vector)\n",
        "\n",
        "        output, hidden = self.gru(embedding_vector)\n",
        "        return output, hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "        # Inputs: ((word_embedding+context), hidden)\n",
        "        self.gru = nn.GRU(hidden_size + hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Inputs: ((hidden+context+input_token))\n",
        "        self.out = nn.Linear(hidden_size + hidden_size + hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "\n",
        "        encoder_context_vector = encoder_hidden\n",
        "\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        predicted_decoder_tokens = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden, encoder_context_vector)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # print(\"Without teacher forcing\")\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "                predicted_decoder_tokens.append(decoder_input)\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, predicted_decoder_tokens # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden_state, encoder_context_vector):\n",
        "        input_token_embedding = self.embedding(input)\n",
        "\n",
        "\n",
        "        input_token_embedding_context_vector = torch.cat((\n",
        "              input_token_embedding,\n",
        "              encoder_context_vector.view(input_token_embedding.shape[0], input_token_embedding.shape[1], encoder_context_vector.shape[2])\n",
        "            ),\n",
        "            dim = 2\n",
        "        )\n",
        "\n",
        "        input_token_embedding_context_vector_relued = F.relu(input_token_embedding_context_vector)\n",
        "\n",
        "\n",
        "        gru_output, gru_hidden = self.gru(input_token_embedding_context_vector_relued, hidden_state)\n",
        "\n",
        "        # FYI - for a single decoder gru call with one timestep, gru_output is the same as the gru_hidden\n",
        "        # print(f\"Gru out and hidden same for one step: {torch.equal(gru_output, gru_hidden.view(input_token_embedding.shape[0], input_token_embedding.shape[1], encoder_context_vector.shape[2]))}\")\n",
        "\n",
        "\n",
        "        gru_output_input_token_embedding_context_vector = torch.cat(\n",
        "            (\n",
        "                input_token_embedding,\n",
        "                gru_output, # == gru_hidden.view(input_token_embedding.shape[0], input_token_embedding.shape[1], encoder_context_vector.shape[2]),\n",
        "                encoder_context_vector.view(input_token_embedding.shape[0], input_token_embedding.shape[1], encoder_context_vector.shape[2])\n",
        "            ),\n",
        "            dim = 2\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        linear_output = self.out(gru_output_input_token_embedding_context_vector)\n",
        "\n",
        "        # print(f\"linear_output shape: {linear_output.shape}\")\n",
        "        # print(f\"gru_hidden shape: {gru_hidden.shape}\")\n",
        "        # return\n",
        "\n",
        "        return linear_output, gru_hidden\n",
        "\n",
        "class EncoderDecoderTranslation(nn.Module):\n",
        "\n",
        "  def __init__(self, input_lang, output_lang, hidden_size, device):\n",
        "        super(EncoderDecoderTranslation, self).__init__()\n",
        "\n",
        "        self.encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "        self.decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "        self.device = device\n",
        "\n",
        "  def forward(self, input_tensor, target_tensor=None):\n",
        "\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
        "    decoder_outputs, _, predicted_decoder_tokens = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "    return decoder_outputs, predicted_decoder_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m6BznnvCuMHZ"
      },
      "outputs": [],
      "source": [
        "def memory_allocation():\n",
        "  print()\n",
        "  # Choose the GPU device (e.g., device 0)\n",
        "  device = torch.device('cuda:0')\n",
        "\n",
        "  # Get the device properties to retrieve the total memory\n",
        "  device_properties = torch.cuda.get_device_properties(device)\n",
        "  total_memory = device_properties.total_memory  # Total memory in bytes\n",
        "\n",
        "  # Get the current memory allocated and reserved\n",
        "  memory_allocated = torch.cuda.memory_allocated(device=device)\n",
        "  memory_reserved = torch.cuda.memory_reserved(device=device)\n",
        "\n",
        "  # Calculate available memory by subtracting allocated and reserved from total\n",
        "  memory_available = total_memory - memory_allocated - memory_reserved\n",
        "\n",
        "  # Convert memory from bytes to MB for better readability\n",
        "  memory_available_mb = memory_available / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "  print(f\"Total GPU memory: {total_memory / (1024 * 1024):.2f} MB\")\n",
        "  print(f\"Current GPU memory allocated: {memory_allocated / (1024 * 1024):.2f} MB\")\n",
        "  print(f\"Current GPU memory reserved: {memory_reserved / (1024 * 1024):.2f} MB\")\n",
        "  print(f\"Estimated available GPU memory: {memory_available_mb:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0riCRmL9v7Il"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder_decoder, encoder_decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in tqdm(dataloader):\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # zero out gradients before each batch\n",
        "        encoder_decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Run encoder-decoder forward()\n",
        "        decoder_outputs, _ = encoder_decoder(input_tensor, target_tensor)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "\n",
        "        # calculate gradients\n",
        "        loss.backward()\n",
        "        # update weights\n",
        "        encoder_decoder_optimizer.step()\n",
        "\n",
        "        # update epoch level loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def normalize_tensors_to_tokens(tensor, remove_first_idx=False):\n",
        "\n",
        "  # convert_to_list\n",
        "  tensor = tensor.tolist()\n",
        "\n",
        "  # remove_sos_eos_and_pads_convert_list\n",
        "  if remove_first_idx:\n",
        "    tensor = [sequence[1:] for sequence in tensor]\n",
        "\n",
        "  # remove all tokens after <eos token>\n",
        "  out_list = []\n",
        "  for sequence in tensor:\n",
        "    new_seq = []\n",
        "    for token in sequence:\n",
        "      if token == EOS_token:\n",
        "        break\n",
        "      new_seq.append(token)\n",
        "    out_list.append(new_seq)\n",
        "\n",
        "  return out_list\n",
        "\n",
        "\n",
        "def predict(data_loader, encoder_decoder):\n",
        "\n",
        "  # Eval Mode. Turn off dropout and batchnorm\n",
        "  encoder_decoder.eval()\n",
        "\n",
        "  list_decoder_outputs = []\n",
        "\n",
        "  # ensure no gradients are calculated with no_grad() to preserve memory\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      input_tensor, target_tensor = data\n",
        "      decoder_outputs, predicted_decoder_tokens = encoder_decoder(input_tensor)\n",
        "      list_decoder_outputs.append(decoder_outputs)\n",
        "\n",
        "      # Merge timesteps of decoder predictions\n",
        "      predicted_decoder_tokens = torch.cat(predicted_decoder_tokens, dim=1)\n",
        "\n",
        "  return list_decoder_outputs, input_tensor, target_tensor, predicted_decoder_tokens\n",
        "\n",
        "\n",
        "def calculate_bleu(test_target_tokens, predicted_decoder_tokens):\n",
        "\n",
        "  return corpus_bleu(\n",
        "      [[item] for item in test_target_tokens],\n",
        "      [item for item in predicted_decoder_tokens],\n",
        "    )\n",
        "\n",
        "def train(train_dataloader, test_dataloader, encoder_decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_decoder_optimizer = optim.Adam(encoder_decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Ignore pad token from loss calculation\n",
        "    if is_ignore_pads:\n",
        "      criterion = nn.NLLLoss(ignore_index = PAD_token)\n",
        "    else:\n",
        "      criterion = nn.NLLLoss()\n",
        "\n",
        "    print('Time \\t\\t\\t (Epoch\\t%) \\t Loss \\t\\t Bleu')\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # memory_allocation()\n",
        "        # Training\n",
        "        encoder_decoder.train()\n",
        "        loss = train_epoch(train_dataloader, encoder_decoder, encoder_decoder_optimizer, criterion)\n",
        "\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        # Eval on Test\n",
        "        encoder_decoder.eval()\n",
        "        # Evaluate without teacher forcing on test set\n",
        "        test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)\n",
        "\n",
        "        # Calculate bleu\n",
        "        bleu = calculate_bleu(\n",
        "            normalize_tensors_to_tokens(test_target_tensor, False),\n",
        "            normalize_tensors_to_tokens(predicted_decoder_tokens, False)\n",
        "        )\n",
        "\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s \\t (%d \\t %d%%) \\t %.4f \\t %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg, bleu))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    Helpers.showPlot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r4NY1Q_Kq77W"
      },
      "outputs": [],
      "source": [
        "# input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNlYWz2zFy2H",
        "outputId": "94f0b8d8-4497-4abb-ce12-739de40a7f13"
      },
      "outputs": [],
      "source": [
        "is_ignore_pads = True\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "\n",
        "# init encoder-decoder\n",
        "encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThKShpL_Z13a"
      },
      "outputs": [],
      "source": [
        "# is_ignore_pads = False\n",
        "# # init encoder-decoder\n",
        "# encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "# train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF3AUb9mmgW2"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MjYBvJQUckH"
      },
      "outputs": [],
      "source": [
        "# Example Predictions without normalizing tokens\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0].tolist()))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1].tolist()))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2].tolist()))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llYgWdxhRQHW"
      },
      "outputs": [],
      "source": [
        "# Normalize tokens\n",
        "test_input_tensor = normalize_tensors_to_tokens(test_input_tensor, False)\n",
        "test_target_tensor = normalize_tensors_to_tokens(test_target_tensor, False)\n",
        "predicted_decoder_tokens = normalize_tensors_to_tokens(predicted_decoder_tokens, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njMlc4YempAE"
      },
      "outputs": [],
      "source": [
        "# Example Predictions with normalizing tokens\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0]))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1]))\n",
        "print()\n",
        "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2]))\n",
        "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66GI9hnzLzX-"
      },
      "outputs": [],
      "source": [
        "is_ignore_pads = False\n",
        "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "\n",
        "# init encoder-decoder\n",
        "encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
        "\n",
        "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMGHE5HRqWgz"
      },
      "source": [
        "# TO TRY\n",
        "- Ignore padding token loss (ignore index) - mixed results\n",
        "- evaluation metric - bleu (done)\n",
        "- EOS token related sequence clipping - done\n",
        "- shifted target sequence? - ignore (done)\n",
        "- loss: what all to include? to include <EOS>? - eos included, padding not included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS7KdW3zDTQO"
      },
      "outputs": [],
      "source": [
        "# def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "#           decoder_optimizer, criterion):\n",
        "\n",
        "#     total_loss = 0\n",
        "#     for data in dataloader:\n",
        "#         input_tensor, target_tensor = data\n",
        "\n",
        "#         encoder_optimizer.zero_grad()\n",
        "#         decoder_optimizer.zero_grad()\n",
        "\n",
        "#         encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "#         decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "#         loss = criterion(\n",
        "#             decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "#             target_tensor.view(-1)\n",
        "#         )\n",
        "#         loss.backward()\n",
        "\n",
        "#         encoder_optimizer.step()\n",
        "#         decoder_optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     return total_loss / len(dataloader)\n",
        "\n",
        "# def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "#                print_every=100, plot_every=100):\n",
        "#     start = time.time()\n",
        "#     plot_losses = []\n",
        "#     print_loss_total = 0  # Reset every print_every\n",
        "#     plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "#     encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "#     decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "#     criterion = nn.NLLLoss()\n",
        "\n",
        "#     for epoch in range(1, n_epochs + 1):\n",
        "#         loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "#         print_loss_total += loss\n",
        "#         plot_loss_total += loss\n",
        "\n",
        "#         if epoch % print_every == 0:\n",
        "#             print_loss_avg = print_loss_total / print_every\n",
        "#             print_loss_total = 0\n",
        "#             print('%s (%d %d%%) %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
        "#                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "#         if epoch % plot_every == 0:\n",
        "#             plot_loss_avg = plot_loss_total / plot_every\n",
        "#             plot_losses.append(plot_loss_avg)\n",
        "#             plot_loss_total = 0\n",
        "\n",
        "#     Helpers.showPlot(plot_losses)\n",
        "\n",
        "# hidden_size = 128\n",
        "# batch_size = 32\n",
        "# input_lang, output_lang, train_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
        "# encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# # train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QpTo7s10fgp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv_10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
