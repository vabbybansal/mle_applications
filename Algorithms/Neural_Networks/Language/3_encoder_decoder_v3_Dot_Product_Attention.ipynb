{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpoJjHRzCfgN"
   },
   "outputs": [],
   "source": [
    "# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\n",
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n",
    "# Comments: https://colab.research.google.com/drive/1NmWujB2PoJk24uOwZ4cAfX3O8cZyigyf\n",
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png\n",
    "# https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7\n",
    "# https://machinelearningmastery.com/the-luong-attention-mechanism/\n",
    "# https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb - Luong attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X1cgGDCC1SU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7Da-Pm0C1ky"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51995,
     "status": "ok",
     "timestamp": 1704134573700,
     "user": {
      "displayName": "Vaibhav Bansal",
      "userId": "03259254199963156033"
     },
     "user_tz": -330
    },
    "id": "hkwI32QnIQlt",
    "outputId": "290e41fb-404d-4036-b25f-4e964c5c3190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "is_ignore_pads = True\n",
    "MAX_LENGTH = 10\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "SPLIT_RATIO = 0.95\n",
    "\n",
    "\n",
    "ENG_PREFIXES = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "# Data location\n",
    "file_path = 'data/eng-fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJQCuQe_C4gD"
   },
   "outputs": [],
   "source": [
    "# Language class handler\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD_token\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "class PreProcess(object):\n",
    "  # Turn a Unicode string to plain ASCII, thanks to\n",
    "  # https://stackoverflow.com/a/518232/2809427\n",
    "  def unicodeToAscii(s):\n",
    "      return ''.join(\n",
    "          c for c in unicodedata.normalize('NFD', s)\n",
    "          if unicodedata.category(c) != 'Mn'\n",
    "      )\n",
    "\n",
    "  # Lowercase, trim, and remove non-letter characters\n",
    "  def normalizeString(s):\n",
    "      s = PreProcess.unicodeToAscii(s.lower().strip())\n",
    "      s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "      s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "      return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muq8MZ6zGkUO"
   },
   "outputs": [],
   "source": [
    "class DataHandler(object):\n",
    "\n",
    "  # read langs and create lang objects, and pairs\n",
    "  def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file_path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[PreProcess.normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "  # filter pairs with length < max length + containing the eng_prefixes as mentioned in eng_prefixes\n",
    "  def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "        # and \\\n",
    "        # p[1].startswith(ENG_PREFIXES)\n",
    "\n",
    "  # filter pairs\n",
    "  def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if DataHandler.filterPair(pair)]\n",
    "\n",
    "  # Read data, filter data, register language objects\n",
    "  def prepareData(lang1, lang2, reverse=False):\n",
    "\n",
    "    # initiate language objects, and get pairs\n",
    "    input_lang, output_lang, pairs = DataHandler.readLangs(lang1, lang2, reverse)\n",
    "\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = DataHandler.filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "\n",
    "    # Register pairs with lang objects\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89jC4beJDXJg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DataLoaderHandler(object):\n",
    "\n",
    "  def sentenceFromIndices(lang, indices):\n",
    "      return ' '.join([lang.index2word[index] for index in indices])\n",
    "\n",
    "  # create a list of token-indices from a list of token\n",
    "  def indexesFromSentence(lang, sentence):\n",
    "      return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "  # create tensor from sentence\n",
    "  def tensorFromSentence(lang, sentence):\n",
    "      indexes = DataLoaderHandler.indexesFromSentence(lang, sentence)\n",
    "      indexes.append(EOS_token)\n",
    "      return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "  # create tensors from pair of sentences\n",
    "  def tensorsFromPair(pair):\n",
    "      input_tensor = DataLoaderHandler.tensorFromSentence(input_lang, pair[0])\n",
    "      target_tensor = DataLoaderHandler.tensorFromSentence(output_lang, pair[1])\n",
    "      return (input_tensor, target_tensor)\n",
    "\n",
    "  def split_train_test(pairs, split_ratio):\n",
    "\n",
    "    # Shuffle the data to ensure randomness\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    # Calculate the split indices\n",
    "    split_idx = int(len(pairs) * split_ratio)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_pairs = pairs[:split_idx]\n",
    "    test_pairs = pairs[split_idx:]\n",
    "\n",
    "    # Optionally, if you want to further use the data as lists instead of references\n",
    "    train_pairs = list(train_pairs)\n",
    "    test_pairs = list(test_pairs)\n",
    "\n",
    "    return train_pairs, test_pairs\n",
    "\n",
    "  def tokenize_into_numpy_arrays(pairs, n, input_lang, output_lang):\n",
    "    # TODO: TRY INPUT AS VARIABLE LENGTH\n",
    "    # Init numpy arrays for timesteps with zeros. Should this be something else other than zeros to mark an empty token? (Since 0 is taken by SOS token)\n",
    "\n",
    "    input_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
    "    target_ids = np.full((n, MAX_LENGTH), PAD_token, dtype=np.int32)\n",
    "    # input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    # target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        # Get list of token-indices\n",
    "        inp_ids = DataLoaderHandler.indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = DataLoaderHandler.indexesFromSentence(output_lang, tgt)\n",
    "\n",
    "        # Append <end of string> tokens\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "\n",
    "        # Assign token indices in the main array\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "    return input_ids, target_ids\n",
    "\n",
    "  # generate data loader\n",
    "  def get_dataloader(batch_size):\n",
    "      # prepare language data\n",
    "      input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
    "\n",
    "      n = len(pairs)\n",
    "\n",
    "      train_pairs, test_pairs = DataLoaderHandler.split_train_test(pairs, SPLIT_RATIO)\n",
    "      n_train, n_test = len(train_pairs), len(test_pairs)\n",
    "\n",
    "      train_input_ids, train_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(train_pairs, n_train, input_lang, output_lang)\n",
    "      train_data = TensorDataset(\n",
    "                      torch.LongTensor(train_input_ids).to(device),\n",
    "                      torch.LongTensor(train_target_ids).to(device)\n",
    "      )\n",
    "\n",
    "      test_input_ids, test_target_ids = DataLoaderHandler.tokenize_into_numpy_arrays(test_pairs, n_test, input_lang, output_lang)\n",
    "      test_data = TensorDataset(\n",
    "                      torch.LongTensor(test_input_ids).to(device),\n",
    "                      torch.LongTensor(test_target_ids).to(device)\n",
    "      )\n",
    "\n",
    "      # Create a sampler\n",
    "      train_sampler = RandomSampler(train_data)\n",
    "      test_sampler = RandomSampler(test_data)\n",
    "\n",
    "      # Create a torch dataloader\n",
    "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "      test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=n_test)\n",
    "\n",
    "      print(f\"Train and Test Dataset # samples: {len(train_data)}, {len(test_data)}\")\n",
    "      print(f\"Train and Test Dataloader # batches: {len(train_dataloader)}, {len(test_dataloader)}\")\n",
    "\n",
    "      return input_lang, output_lang, train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11342,
     "status": "ok",
     "timestamp": 1704134585038,
     "user": {
      "displayName": "Vaibhav Bansal",
      "userId": "03259254199963156033"
     },
     "user_tz": -330
    },
    "id": "sJoyZiRuojNA",
    "outputId": "e32d5a77-ccbd-422e-9dcf-1272cb107544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 105692 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 17865\n",
      "eng 10699\n",
      "Train and Test Dataset # samples: 100407, 5285\n",
      "Train and Test Dataloader # batches: 3138, 1\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "# input_lang, output_lang, pairs = DataHandler.prepareData('eng', 'fra', True)\n",
    "# print(random.choice(pairs))\n",
    "\n",
    "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCOFSxKeDkEB"
   },
   "source": [
    "**Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNGIszhqDjle"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "class Helpers(object):\n",
    "\n",
    "  def asMinutes(s):\n",
    "      m = math.floor(s / 60)\n",
    "      s -= m * 60\n",
    "      return '%dm %ds' % (m, s)\n",
    "\n",
    "  def timeSince(since, percent):\n",
    "      now = time.time()\n",
    "      s = now - since\n",
    "      es = s / (percent)\n",
    "      rs = es - s\n",
    "      return '%s (- %s)' % (Helpers.asMinutes(s), Helpers.asMinutes(rs))\n",
    "\n",
    "  def showPlot(points):\n",
    "      plt.figure()\n",
    "      fig, ax = plt.subplots()\n",
    "      # this locator puts ticks at regular intervals\n",
    "      loc = ticker.MultipleLocator(base=0.2)\n",
    "      ax.yaxis.set_major_locator(loc)\n",
    "      plt.plot(points)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qViMMrjBDnlZ"
   },
   "source": [
    "The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQZI4vkXDJ2E"
   },
   "outputs": [],
   "source": [
    "class GeneralDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GeneralDotProductAttention, self).__init__()\n",
    "        self.W_encoder_states = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, keys, query):\n",
    "\n",
    "        query = query.permute(1,2,0)\n",
    "        keys = self.W_encoder_states(keys)\n",
    "\n",
    "        dot_product = torch.matmul(keys, query) / torch.rsqrt(torch.tensor(keys.shape[-1], dtype=torch.float32))\n",
    "\n",
    "        weights = F.softmax(dot_product, dim=1)\n",
    "\n",
    "#         print(weights.shape)\n",
    "        element_wise_multiplication = keys * weights\n",
    "\n",
    "        context_vector = torch.sum(element_wise_multiplication, dim=1, keepdim=True)\n",
    "\n",
    "        # context_alternative = torch.bmm(weights.permute(0,2,1), keys)\n",
    "        # print(f\"context_alternative shape: {context_alternative.shape}\")\n",
    "        # print(f\"context_v1: {context_alternative}\")\n",
    "\n",
    "        return context_vector, weights.permute(0,2,1)\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, keys, query):\n",
    "\n",
    "        query = query.permute(1,2,0)\n",
    "\n",
    "        dot_product = torch.matmul(keys, query) / torch.rsqrt(torch.tensor(keys.shape[-1], dtype=torch.float32))\n",
    "#         key_dim = encoder_outputs.size(-1)\n",
    "#         # Scale the dot product\n",
    "#         scaled_dot_product = dot_product / torch.sqrt(torch.tensor(key_dim, dtype=torch.float32))\n",
    "\n",
    "        weights = F.softmax(dot_product, dim=1)\n",
    "\n",
    "#         print(weights.shape)\n",
    "        element_wise_multiplication = keys * weights\n",
    "\n",
    "        context_vector = torch.sum(element_wise_multiplication, dim=1, keepdim=True)\n",
    "\n",
    "        # context_alternative = torch.bmm(weights.permute(0,2,1), keys)\n",
    "        # print(f\"context_alternative shape: {context_alternative.shape}\")\n",
    "        # print(f\"context_v1: {context_alternative}\")\n",
    "\n",
    "        return context_vector, weights.permute(0,2,1)\n",
    "\n",
    "\n",
    "class DotProductAttentionEncoderRNN2(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout_p=0.1):\n",
    "        super(DotProductAttentionEncoderRNN2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Convert to embedding {vocab_size, embedding_dimension: hidden_size}\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Embedding vector\n",
    "        embedding_vector = self.embedding(input)\n",
    "        embedding_vector = self.dropout(embedding_vector)\n",
    "\n",
    "        output, hidden = self.gru(embedding_vector)\n",
    "        return output, hidden\n",
    "\n",
    "class DotProductAttentionDecoderRNN2(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DotProductAttentionDecoderRNN2, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dot_product_attention = DotProductAttention()\n",
    "\n",
    "        # Inputs: ((word_embedding+context), hidden)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # self.pre_out = nn.Linear(hidden_size + hidden_size, hidden_size + hidden_size)\n",
    "\n",
    "        # Inputs: ((hidden+context+input_token))\n",
    "        self.out = nn.Linear(hidden_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "\n",
    "        encoder_context_vector = encoder_hidden\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        predicted_decoder_tokens = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden, encoder_context_vector, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # print(\"Without teacher forcing\")\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                predicted_decoder_tokens.append(decoder_input)\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, predicted_decoder_tokens # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, prev_hidden_state, encoder_context_vector, encoder_outputs):\n",
    "\n",
    "        # input embedding vector\n",
    "        input_token_embedding = self.embedding(input)\n",
    "\n",
    "        input_token_embedding_relued = F.relu(input_token_embedding)\n",
    "\n",
    "\n",
    "        gru_output, gru_hidden = self.gru(input_token_embedding_relued, prev_hidden_state)\n",
    "\n",
    "        # Dot Product Attention\n",
    "        context_vector, _ = self.dot_product_attention(encoder_outputs, gru_hidden)\n",
    "\n",
    "#         print(f\"gru_hidden shape: {gru_hidden.shape}\")\n",
    "#         print(f\"context_vector shape: {context_vector.shape}\")\n",
    "        linear_output = torch.cat(\n",
    "            (\n",
    "                gru_hidden,\n",
    "                context_vector.permute(1,0,2)\n",
    "            ),\n",
    "            2\n",
    "        )\n",
    "#         print(f\"decoder_hidden_concatenate_attended_context {linear_output.shape}\")\n",
    "\n",
    "        # linear_output = self.pre_out(linear_output)\n",
    "#         print(f\"linear after pre_output {linear_output.shape}\")\n",
    "\n",
    "        # linear_output = F.relu(linear_output)\n",
    "\n",
    "#         print(f\"linear after relu_output {linear_output.shape}\")\n",
    "\n",
    "        linear_output = self.out(linear_output)\n",
    "\n",
    "        linear_output = linear_output.view(linear_output.shape[1], linear_output.shape[0], linear_output.shape[2])\n",
    "\n",
    "\n",
    "        return linear_output, gru_hidden\n",
    "\n",
    "class DotProductAttentionEncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_lang, output_lang, hidden_size, device):\n",
    "        super(DotProductAttentionEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = DotProductAttentionEncoderRNN2(input_lang.n_words, hidden_size).to(device)\n",
    "        self.decoder = DotProductAttentionDecoderRNN2(hidden_size, output_lang.n_words).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor=None):\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
    "        decoder_outputs, _, predicted_decoder_tokens = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        return decoder_outputs, predicted_decoder_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQhE8J7Va0Wx"
   },
   "outputs": [],
   "source": [
    "# \"General\" as mentioned here - https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n",
    "\n",
    "\n",
    "# SCALED DOT PRODUCT\n",
    "# class GeneralDotProductAttention(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GeneralDotProductAttention, self).__init__()\n",
    "\n",
    "#     def forward(self, keys, query):\n",
    "\n",
    "#         dot_product = torch.matmul(keys, query)\n",
    "#         dot_product_norm = F.softmax(dot_product, dim=1)\n",
    "#         element_wise_multiplication = keys * dot_product_norm\n",
    "\n",
    "#         context_vector = torch.sum(element_wise_multiplication, dim=1)\n",
    "\n",
    "#         return context_vector\n",
    "\n",
    "\n",
    "class GeneralDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GeneralDotProductAttention, self).__init__()\n",
    "        self.W_encoder_states = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, keys, query):\n",
    "\n",
    "        query = query.permute(1,2,0)\n",
    "        keys = self.W_encoder_states(keys)\n",
    "\n",
    "        dot_product = torch.matmul(keys, query) / torch.rsqrt(torch.tensor(keys.shape[-1], dtype=torch.float32))\n",
    "#         key_dim = encoder_outputs.size(-1)\n",
    "#         # Scale the dot product\n",
    "#         scaled_dot_product = dot_product / torch.sqrt(torch.tensor(key_dim, dtype=torch.float32))\n",
    "\n",
    "        weights = F.softmax(dot_product, dim=1)\n",
    "\n",
    "#         print(weights.shape)\n",
    "        element_wise_multiplication = keys * weights\n",
    "\n",
    "        context_vector = torch.sum(element_wise_multiplication, dim=1, keepdim=True)\n",
    "\n",
    "        # context_alternative = torch.bmm(weights.permute(0,2,1), keys)\n",
    "        # print(f\"context_alternative shape: {context_alternative.shape}\")\n",
    "        # print(f\"context_v1: {context_alternative}\")\n",
    "\n",
    "        return context_vector, weights.permute(0,2,1)\n",
    "\n",
    "\n",
    "class GeneralDotProductAttentionEncoderRNN2(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout_p=0.1):\n",
    "        super(GeneralDotProductAttentionEncoderRNN2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Convert to embedding {vocab_size, embedding_dimension: hidden_size}\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Embedding vector\n",
    "        embedding_vector = self.embedding(input)\n",
    "        embedding_vector = self.dropout(embedding_vector)\n",
    "\n",
    "        output, hidden = self.gru(embedding_vector)\n",
    "        return output, hidden\n",
    "\n",
    "class GeneralDotProductAttentionDecoderRNN2(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(GeneralDotProductAttentionDecoderRNN2, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dot_product_attention = GeneralDotProductAttention(hidden_size)\n",
    "\n",
    "        # Inputs: ((word_embedding+context), hidden)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Inputs: ((hidden+context+input_token))\n",
    "        self.out = nn.Linear(hidden_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "\n",
    "        encoder_context_vector = encoder_hidden\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        predicted_decoder_tokens = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden, encoder_context_vector, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # print(\"Without teacher forcing\")\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "                predicted_decoder_tokens.append(decoder_input)\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, predicted_decoder_tokens # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, prev_hidden_state, encoder_context_vector, encoder_outputs):\n",
    "\n",
    "        # input embedding vector\n",
    "        input_token_embedding = self.embedding(input)\n",
    "\n",
    "        input_token_embedding_relued = F.relu(input_token_embedding)\n",
    "\n",
    "\n",
    "        gru_output, gru_hidden = self.gru(input_token_embedding_relued, prev_hidden_state)\n",
    "\n",
    "        # Dot Product Attention\n",
    "        context_vector, _ = self.dot_product_attention(encoder_outputs, gru_hidden)\n",
    "\n",
    "#         print(f\"gru_hidden shape: {gru_hidden.shape}\")\n",
    "#         print(f\"context_vector shape: {context_vector.shape}\")\n",
    "        linear_output = torch.cat(\n",
    "            (\n",
    "                gru_hidden,\n",
    "                context_vector.permute(1,0,2)\n",
    "            ),\n",
    "            2\n",
    "        )\n",
    "#         print(f\"decoder_hidden_concatenate_attended_context {linear_output.shape}\")\n",
    "\n",
    "#         linear_output = self.pre_out(linear_output)\n",
    "# #         print(f\"linear after pre_output {linear_output.shape}\")\n",
    "\n",
    "#         linear_output = F.relu(linear_output)\n",
    "\n",
    "#         print(f\"linear after relu_output {linear_output.shape}\")\n",
    "\n",
    "        linear_output = self.out(linear_output)\n",
    "\n",
    "        linear_output = linear_output.view(linear_output.shape[1], linear_output.shape[0], linear_output.shape[2])\n",
    "\n",
    "\n",
    "        return linear_output, gru_hidden\n",
    "\n",
    "class GeneralDotProductAttentionEncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_lang, output_lang, hidden_size, device):\n",
    "        super(GeneralDotProductAttentionEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.encoder = GeneralDotProductAttentionEncoderRNN2(input_lang.n_words, hidden_size).to(device)\n",
    "        self.decoder = GeneralDotProductAttentionDecoderRNN2(hidden_size, output_lang.n_words).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor=None):\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_tensor)\n",
    "        decoder_outputs, _, predicted_decoder_tokens = self.decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        return decoder_outputs, predicted_decoder_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0riCRmL9v7Il"
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder_decoder, encoder_decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        # zero out gradients before each batch\n",
    "        encoder_decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Run encoder-decoder forward()\n",
    "        decoder_outputs, _ = encoder_decoder(input_tensor, target_tensor)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        encoder_decoder_optimizer.step()\n",
    "\n",
    "        # update epoch level loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def normalize_tensors_to_tokens(tensor, remove_first_idx=False):\n",
    "\n",
    "  # convert_to_list\n",
    "  tensor = tensor.tolist()\n",
    "\n",
    "  # remove_sos_eos_and_pads_convert_list\n",
    "  if remove_first_idx:\n",
    "    tensor = [sequence[1:] for sequence in tensor]\n",
    "\n",
    "  # remove all tokens after <eos token>\n",
    "  out_list = []\n",
    "  for sequence in tensor:\n",
    "    new_seq = []\n",
    "    for token in sequence:\n",
    "      if token == EOS_token:\n",
    "        break\n",
    "      new_seq.append(token)\n",
    "    out_list.append(new_seq)\n",
    "\n",
    "  return out_list\n",
    "\n",
    "\n",
    "def predict(data_loader, encoder_decoder):\n",
    "\n",
    "  # Eval Mode. Turn off dropout and batchnorm\n",
    "  encoder_decoder.eval()\n",
    "\n",
    "  list_decoder_outputs = []\n",
    "\n",
    "  # ensure no gradients are calculated with no_grad() to preserve memory\n",
    "  with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "      input_tensor, target_tensor = data\n",
    "      decoder_outputs, predicted_decoder_tokens = encoder_decoder(input_tensor)\n",
    "      list_decoder_outputs.append(decoder_outputs)\n",
    "\n",
    "      # Merge timesteps of decoder predictions\n",
    "      predicted_decoder_tokens = torch.cat(predicted_decoder_tokens, dim=1)\n",
    "\n",
    "  return list_decoder_outputs, input_tensor, target_tensor, predicted_decoder_tokens\n",
    "\n",
    "\n",
    "def calculate_bleu(test_target_tokens, predicted_decoder_tokens):\n",
    "\n",
    "  return corpus_bleu(\n",
    "      [[item] for item in test_target_tokens],\n",
    "      [item for item in predicted_decoder_tokens],\n",
    "    )\n",
    "\n",
    "def train(train_dataloader, test_dataloader, encoder_decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_decoder_optimizer = optim.Adam(encoder_decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Ignore pad token from loss calculation\n",
    "    if is_ignore_pads:\n",
    "      criterion = nn.NLLLoss(ignore_index = PAD_token)\n",
    "    else:\n",
    "      criterion = nn.NLLLoss()\n",
    "\n",
    "    print('Time \\t\\t\\t (Epoch\\t%) \\t Loss \\t\\t Bleu')\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training\n",
    "        encoder_decoder.train()\n",
    "        loss = train_epoch(train_dataloader, encoder_decoder, encoder_decoder_optimizer, criterion)\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        # Eval on Test\n",
    "        encoder_decoder.eval()\n",
    "        # Evaluate without teacher forcing on test set\n",
    "        test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)\n",
    "\n",
    "        # Calculate bleu\n",
    "        bleu = calculate_bleu(\n",
    "            normalize_tensors_to_tokens(test_target_tensor, False),\n",
    "            normalize_tensors_to_tokens(predicted_decoder_tokens, False)\n",
    "        )\n",
    "\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s \\t (%d \\t %d%%) \\t %.4f \\t %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg, bleu))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    Helpers.showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4NY1Q_Kq77W",
    "outputId": "9f63a9dd-2a2d-4335-89bb-04088440f467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 105692 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 17865\n",
      "eng 10699\n",
      "Train and Test Dataset # samples: 100407, 5285\n",
      "Train and Test Dataloader # batches: 1569, 1\n",
      "Time \t\t\t (Epoch\t%) \t Loss \t\t Bleu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:32<00:00, 48.21it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.83it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 49.69it/s]\n",
      "100%|██████████| 1569/1569 [00:32<00:00, 48.46it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 43s (- 106m 32s) \t (5 \t 2%) \t 2.2345 \t 0.2963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.46it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.21it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.18it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.11it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 21s (- 101m 45s) \t (10 \t 5%) \t 1.1234 \t 0.3581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.30it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.72it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.62it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.13it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7m 59s (- 98m 32s) \t (15 \t 7%) \t 0.8337 \t 0.4021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.33it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.53it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.23it/s]\n",
      "100%|██████████| 1569/1569 [00:33<00:00, 47.52it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 39s (- 95m 56s) \t (20 \t 10%) \t 0.6836 \t 0.4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.08it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.29it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.69it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.47it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13m 17s (- 93m 3s) \t (25 \t 12%) \t 0.5849 \t 0.4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 50.96it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.28it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.10it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.57it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 55s (- 90m 13s) \t (30 \t 15%) \t 0.5174 \t 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.23it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.96it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.93it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.42it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18m 33s (- 87m 27s) \t (35 \t 17%) \t 0.4639 \t 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.36it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.90it/s]\n",
      "100%|██████████| 1569/1569 [00:32<00:00, 47.80it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.87it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 12s (- 84m 51s) \t (40 \t 20%) \t 0.4244 \t 0.4484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.20it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.58it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.05it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.84it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 51s (- 82m 10s) \t (45 \t 22%) \t 0.3917 \t 0.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.34it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 49.85it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.03it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.15it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26m 30s (- 79m 30s) \t (50 \t 25%) \t 0.3645 \t 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.04it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.09it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.69it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.36it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29m 8s (- 76m 49s) \t (55 \t 27%) \t 0.3422 \t 0.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.02it/s]\n",
      "100%|██████████| 1569/1569 [00:32<00:00, 48.83it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.73it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.47it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31m 48s (- 74m 12s) \t (60 \t 30%) \t 0.3248 \t 0.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.04it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.34it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.03it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.71it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 50.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34m 26s (- 71m 31s) \t (65 \t 32%) \t 0.3086 \t 0.4704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.07it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.00it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.35it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.32it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37m 3s (- 68m 48s) \t (70 \t 35%) \t 0.2918 \t 0.4718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:31<00:00, 50.10it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.96it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.40it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.33it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39m 40s (- 66m 7s) \t (75 \t 37%) \t 0.2807 \t 0.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:32<00:00, 48.44it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.78it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.32it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.42it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42m 19s (- 63m 29s) \t (80 \t 40%) \t 0.2695 \t 0.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.43it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.08it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.02it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.45it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44m 56s (- 60m 47s) \t (85 \t 42%) \t 0.2570 \t 0.4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.23it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.49it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.49it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.04it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 50.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47m 33s (- 58m 7s) \t (90 \t 45%) \t 0.2501 \t 0.4837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.34it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.25it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.69it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.02it/s]\n",
      "100%|██████████| 1569/1569 [00:31<00:00, 49.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50m 11s (- 55m 28s) \t (95 \t 47%) \t 0.2408 \t 0.4843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 51.73it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 53.11it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.84it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.72it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52m 44s (- 52m 44s) \t (100 \t 50%) \t 0.2342 \t 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:29<00:00, 52.97it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.95it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 52.26it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 52.02it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55m 17s (- 50m 1s) \t (105 \t 52%) \t 0.2264 \t 0.4905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:29<00:00, 53.16it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.97it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.90it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 53.13it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 52.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57m 50s (- 47m 19s) \t (110 \t 55%) \t 0.2213 \t 0.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [00:30<00:00, 52.01it/s]\n",
      "100%|██████████| 1569/1569 [00:29<00:00, 53.08it/s]\n",
      "100%|██████████| 1569/1569 [00:30<00:00, 51.20it/s]\n",
      " 67%|██████▋   | 1053/1569 [00:20<00:12, 42.21it/s]"
     ]
    }
   ],
   "source": [
    "is_ignore_pads = True\n",
    "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
    "\n",
    "# init encoder-decoder\n",
    "encoder_decoder = DotProductAttentionEncoderDecoder(input_lang, output_lang, hidden_size, device)\n",
    "\n",
    "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4vi8p0O8Tui"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def show_model_layers_and_params(model):\n",
    "    print(\"Model Layers:\")\n",
    "    print(\"--------------\")\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"{name}: {module}\")\n",
    "\n",
    "    print(\"\\nLayer-wise Number of Parameters and Memory Requirements:\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    total_params = 0\n",
    "    total_memory = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            param_memory = num_params * param.element_size() / (1024 ** 2)  # Memory in MBs\n",
    "            print(f\"{name}: {num_params} parameters, {param_memory:.2f} MB\")\n",
    "            total_params += num_params\n",
    "            total_memory += param_memory\n",
    "\n",
    "    print(\"\\nTotal Number of Parameters and Memory Usage:\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Total memory usage: {total_memory:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQBWv-ea3hK9"
   },
   "outputs": [],
   "source": [
    "show_model_layers_and_params(encoder_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5sWS7dw3km-"
   },
   "source": [
    "- Does dot product attention increase the number of params?\n",
    "- Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcwyQ16V8yUO"
   },
   "outputs": [],
   "source": [
    "# torch.sum(torch.tensor([[2.3256e-06],\n",
    "#         [3.7920e-06],\n",
    "#         [4.7707e-06],\n",
    "#         [1.0259e-06],\n",
    "#         [9.6182e-06],\n",
    "#         [1.1358e-06],\n",
    "#         [1.2249e-03],\n",
    "#         [2.4842e-02],\n",
    "#         [2.2094e-01],\n",
    "#         [7.5297e-01]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThKShpL_Z13a"
   },
   "outputs": [],
   "source": [
    "# is_ignore_pads = False\n",
    "# # init encoder-decoder\n",
    "# encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
    "\n",
    "# train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF3AUb9mmgW2"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_list_decoder_outputs, test_input_tensor, test_target_tensor, predicted_decoder_tokens = predict(test_dataloader, encoder_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MjYBvJQUckH"
   },
   "outputs": [],
   "source": [
    "# Example Predictions without normalizing tokens\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0].tolist()))\n",
    "print()\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1].tolist()))\n",
    "print()\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2].tolist()))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llYgWdxhRQHW"
   },
   "outputs": [],
   "source": [
    "# Normalize tokens\n",
    "test_input_tensor = normalize_tensors_to_tokens(test_input_tensor, False)\n",
    "test_target_tensor = normalize_tensors_to_tokens(test_target_tensor, False)\n",
    "predicted_decoder_tokens = normalize_tensors_to_tokens(predicted_decoder_tokens, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njMlc4YempAE"
   },
   "outputs": [],
   "source": [
    "# Example Predictions with normalizing tokens\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[0]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[0]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[0]))\n",
    "print()\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[1]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[1]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[1]))\n",
    "print()\n",
    "print(DataLoaderHandler.sentenceFromIndices(input_lang, test_input_tensor[2]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, test_target_tensor[2]))\n",
    "print(DataLoaderHandler.sentenceFromIndices(output_lang, predicted_decoder_tokens[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66GI9hnzLzX-"
   },
   "outputs": [],
   "source": [
    "is_ignore_pads = False\n",
    "input_lang, output_lang, train_dataloader, test_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
    "\n",
    "# init encoder-decoder\n",
    "encoder_decoder = EncoderDecoderTranslation(input_lang, output_lang, hidden_size, device)\n",
    "\n",
    "train(train_dataloader, test_dataloader, encoder_decoder, epochs, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMGHE5HRqWgz"
   },
   "source": [
    "# TO TRY\n",
    "- Ignore padding token loss (ignore index) - mixed results\n",
    "- evaluation metric - bleu (done)\n",
    "- EOS token related sequence clipping - done\n",
    "- shifted target sequence? - ignore (done)\n",
    "- loss: what all to include? to include <EOS>? - eos included, padding not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS7KdW3zDTQO"
   },
   "outputs": [],
   "source": [
    "# def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "#           decoder_optimizer, criterion):\n",
    "\n",
    "#     total_loss = 0\n",
    "#     for data in dataloader:\n",
    "#         input_tensor, target_tensor = data\n",
    "\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "\n",
    "#         encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "#         decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "#         loss = criterion(\n",
    "#             decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "#             target_tensor.view(-1)\n",
    "#         )\n",
    "#         loss.backward()\n",
    "\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "#                print_every=100, plot_every=100):\n",
    "#     start = time.time()\n",
    "#     plot_losses = []\n",
    "#     print_loss_total = 0  # Reset every print_every\n",
    "#     plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "#     encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "#     decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.NLLLoss()\n",
    "\n",
    "#     for epoch in range(1, n_epochs + 1):\n",
    "#         loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#         print_loss_total += loss\n",
    "#         plot_loss_total += loss\n",
    "\n",
    "#         if epoch % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('%s (%d %d%%) %.4f' % (Helpers.timeSince(start, epoch / n_epochs),\n",
    "#                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "#         if epoch % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n",
    "#     Helpers.showPlot(plot_losses)\n",
    "\n",
    "# hidden_size = 128\n",
    "# batch_size = 32\n",
    "# input_lang, output_lang, train_dataloader = DataLoaderHandler.get_dataloader(batch_size)\n",
    "# encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "# decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "# # train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QpTo7s10fgp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN3HfMP+gxPDWbaPrbLGNNd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
