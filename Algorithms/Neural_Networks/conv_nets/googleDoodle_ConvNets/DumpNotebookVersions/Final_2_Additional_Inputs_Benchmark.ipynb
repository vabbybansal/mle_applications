{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, _imaging\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "history = History()\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSampleImgs():\n",
    "    n_samp = 3\n",
    "    train_df_sample = train_df.sample(n_samp)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    for i in range(n_samp):\n",
    "        draw = train_df_sample.iloc[i]['drawing']\n",
    "        label = train_df_sample.iloc[i]['word']\n",
    "        plt.subplot(n_samp,1,i+1)\n",
    "        for stroke in draw:\n",
    "            plt.plot(stroke[0], stroke[1], marker='.', color='black')\n",
    "            plt.title(label)\n",
    "            plt.axis('off')\n",
    "    plt.show()    \n",
    "    \n",
    "# Show an image from the dataframe\n",
    "def showImgFromDf(df, index):\n",
    "    # Show an image\n",
    "    plt.imshow(df.iloc[index]['img'],cmap='gray')\n",
    "    plt.title(df.iloc[index]['word'])\n",
    "    plt.show()\n",
    "    \n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "\n",
    "def baseline_conv_model(num_filters, num_classes, img_height, img_width):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_filters*1, (5,5), input_shape=(img_height,img_width,1), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(num_filters*2, (5,5), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(num_filters*4, (3,3), activation='relu')) \n",
    "#     model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#     model.add(Conv2D(num_filters*8, (3,3), activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(2500, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(2000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1500, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(Dense(num_classes, activation='softmax'))\n",
    "    # model.add(Activation('softmax'))\n",
    "    model.add(Dense(units=num_classes))\n",
    "    model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', top_3_accuracy, 'categorical_crossentropy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111)\n",
    "\n",
    "# Generate class mappings\n",
    "fnames = glob('train_simplified/*.csv')\n",
    "tempDf = pd.DataFrame(columns=pd.read_csv(fnames[0], nrows=1).columns)\n",
    "\n",
    "for name in fnames:\n",
    "    data = pd.read_csv(name, nrows=2)\n",
    "    tempDf = tempDf.append(data)\n",
    "    \n",
    "ys = tempDf['word']\n",
    "    \n",
    "# Convert class labels from categorical to numerical\n",
    "unique_classes_list = ys.unique()\n",
    "map_class_to_numeric = {k: v for v, k in enumerate(unique_classes_list)}\n",
    "map_numeric_to_class = {v: k for k, v in map_class_to_numeric.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_it(raw_strokes, output_height, output_width):\n",
    "    raw_strokes = eval(raw_strokes)\n",
    "    image = Image.new(\"P\", (255,255)\n",
    "#                       , color=1\n",
    "            )\n",
    "    image_draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0])-1):\n",
    "\n",
    "            image_draw.line([stroke[0][i], \n",
    "                             stroke[1][i],\n",
    "                             stroke[0][i+1], \n",
    "                             stroke[1][i+1]],\n",
    "                            fill=255, width=6)\n",
    "    # Reduce image size\n",
    "    image = image.resize((output_height,output_width),Image.ANTIALIAS)\n",
    "    \n",
    "    return np.array(image)\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, num_samples_to_train, batch_size, trainGeneratorDataOffsetPerClass, img_width, img_height):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_to_train = num_samples_to_train\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.trainGeneratorDataOffsetPerClass = trainGeneratorDataOffsetPerClass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.num_samples_to_train / float(self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        numRowsEachDf = int(self.batch_size / float(numClasses))\n",
    "        skipRows = (idx * numRowsEachDf) + self.trainGeneratorDataOffsetPerClass\n",
    "        img_height = self.img_height \n",
    "        img_width = self.img_width\n",
    "        \n",
    "        X, y = DataGenerator._getNextData(numRowsEachDf, skipRows, img_height, img_width)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def _getNextData(numRowsEachDf, skipRows, img_height, img_width):\n",
    "        \n",
    "        fnames = glob('train_simplified/*.csv')\n",
    "        numClasses = len(fnames)\n",
    "\n",
    "        # Get n rows from all the csv files and append them into one dataframe\n",
    "        train_df = pd.DataFrame(columns=pd.read_csv(fnames[0], nrows=1).columns)\n",
    "        \n",
    "        for name in fnames:\n",
    "            if skipRows == 0:\n",
    "                data = pd.read_csv(name, nrows=numRowsEachDf)\n",
    "            else:\n",
    "                data = pd.read_csv(name, nrows=numRowsEachDf, skiprows=range(1,skipRows)) # 0th row is the header, so not skipping that\n",
    "            # Append the training data of all the classes\n",
    "            train_df = train_df.append(data)\n",
    "            \n",
    "        train_df = train_df.reset_index().drop('index', axis=1)\n",
    "\n",
    "        # Shuffle all data points\n",
    "        train_df = train_df.sample(train_df.shape[0])\n",
    "\n",
    "        # Get only those which were correctly recognized\n",
    "        train_df = train_df[train_df['recognized'] == True]\n",
    "\n",
    "        # Convert drawing to images\n",
    "        train_df['drawing'] = train_df['drawing'].apply(lambda x: draw_it(x, img_height, img_width))\n",
    "\n",
    "        # Reshape the array\n",
    "        train_imgArr = np.vstack(train_df['drawing'].values).flatten().reshape((train_df['drawing'].shape[0], img_height, img_width))\n",
    "    \n",
    "        # In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [width][height][pixels] for TF.\n",
    "        # In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In this case, the pixel values are gray scale, the pixel dimension is set to 1.\n",
    "        train_X = train_imgArr.reshape(train_imgArr.shape[0], img_height, img_width, 1).astype('float32')\n",
    "        \n",
    "        \n",
    "         # Initialize the y_train\n",
    "        y_train = train_df['word']\n",
    "    \n",
    "        y_train_numeric = y_train.apply(lambda x: map_class_to_numeric[x])\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        train_y = keras.utils.to_categorical(y_train_numeric, numClasses)\n",
    "        \n",
    "        return train_X, train_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = img_height = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = DataGenerator._getNextData(100, 90000, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp = []\n",
    "batch_size = 1000\n",
    "numTrainExamplesPerClass = 500\n",
    "numClasses = 340\n",
    "num_samples_to_train = (numTrainExamplesPerClass*numClasses)\n",
    "trainGeneratorDataOffsetPerClass = 0\n",
    "dataGenerator = DataGenerator(num_samples_to_train, batch_size, trainGeneratorDataOffsetPerClass, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 431s 3s/step - loss: 6.0701 - acc: 0.0148 - top_3_accuracy: 0.0390 - categorical_crossentropy: 6.0701 - val_loss: 4.9421 - val_acc: 0.0535 - val_top_3_accuracy: 0.1256 - val_categorical_crossentropy: 4.9421\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 427s 3s/step - loss: 4.7927 - acc: 0.0595 - top_3_accuracy: 0.1344 - categorical_crossentropy: 4.7927 - val_loss: 4.3504 - val_acc: 0.1181 - val_top_3_accuracy: 0.2378 - val_categorical_crossentropy: 4.3504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131ca2a50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = baseline_conv_model(4, 340, 32, 32)\n",
    "numStartFilters = 8\n",
    "model = baseline_conv_model(numStartFilters, numClasses, img_height, img_width)\n",
    "model.fit_generator(\n",
    "        generator=dataGenerator,\n",
    "        epochs=2,\n",
    "        verbose=1,\n",
    "        shuffle=False,\n",
    "        validation_data=(X_test, y_test)\n",
    "    \n",
    "#         ,\n",
    "#         use_multiprocessing=True,\n",
    ")\n",
    "#       steps_per_epoch=(num_training_samples // batch_size),\n",
    "#       validation_data=my_validation_batch_generator,\n",
    "#       validation_steps=(num_validation_samples // batch_size),\n",
    "#       use_multiprocessing=False,\n",
    "#       workers=16,\n",
    "#       max_queue_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Error: 29.11%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Transform data for CNN\n",
    "# X_train, X_test, y_train, y_test, imgArr_test, map_class_to_numeric, map_numeric_to_class = CNN_dataPrep(train_df, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_filters = 2\n",
    "# num_classes = 340\n",
    "# continueTrain = False\n",
    "# if continueTrain == False:\n",
    "#     model = baseline_conv_model(num_filters, num_classes, img_height, img_width)\n",
    "\n",
    "# # checkpoint\n",
    "# filepath=\"Saved_Models/weights.best.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# hist = model.fit(X_train, \n",
    "#           y_train, \n",
    "#           validation_data=(X_test, y_test),\n",
    "#           epochs=2, \n",
    "#           batch_size=5, \n",
    "#           verbose=2,\n",
    "#           callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open((\"Saved_Models/model_\" + str(datetime.today())[:16].replace(\" \", \"_\").replace(\":\",\"-\") + \".json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights((\"Saved_Models/model_\" + str(datetime.today())[:16].replace(\" \", \"_\").replace(\":\",\"-\") + \".h5\"))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load the model back\n",
    "# load json and create model\n",
    "json_file = open('Saved_Models/model_2018-11-21_02-44.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Saved_Models/model_2018-11-21_02-44.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', top_3_accuracy, 'categorical_crossentropy'])\n",
    "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestData(img_height, img_width):\n",
    "        \n",
    "        numClasses = 340\n",
    "\n",
    "        test_df = pd.read_csv('test_simplified.csv')            \n",
    "\n",
    "        # Convert drawing to images\n",
    "        test_df['drawing'] = test_df['drawing'].apply(lambda x: draw_it(x, img_height, img_width))\n",
    "\n",
    "        # Reshape the array\n",
    "        test_imgArr = np.vstack(test_df['drawing'].values).flatten().reshape((test_df['drawing'].shape[0], img_height, img_width))\n",
    "    \n",
    "        # In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [width][height][pixels] for TF.\n",
    "        # In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In this case, the pixel values are gray scale, the pixel dimension is set to 1.\n",
    "        test_X = test_imgArr.reshape(test_imgArr.shape[0], img_height, img_width, 1).astype('float32')\n",
    "                \n",
    "        return test_df, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, test_X = getTestData(img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112199/112199 [00:22<00:00, 5018.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "preds = model.predict(test_X)\n",
    "outputDf = test_df.copy()[['key_id']]\n",
    "outputDf['word'] = ''\n",
    "map_numeric_to_class_space_normal = map_numeric_to_class.copy()\n",
    "for key in map_numeric_to_class_space_normal:\n",
    "    map_numeric_to_class_space_normal[key] = (map_numeric_to_class_space_normal[key].replace(\" \", \"_\"))\n",
    "\n",
    "for i in tqdm(range(preds.shape[0])):\n",
    "    outputDf['word'].at[i] = ' '.join(([map_numeric_to_class_space_normal[predClass] for predClass in [tup[1] for tup in sorted(zip(preds[i], range(340)), reverse=True)[:3]]]))\n",
    "\n",
    "# Create csv\n",
    "outputDf.to_csv('initial_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
