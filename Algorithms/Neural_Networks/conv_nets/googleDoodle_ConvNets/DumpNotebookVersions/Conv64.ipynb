{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, _imaging\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "history = History()\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainData(output_height, output_width, recordsPerTrainClass, skiprows=0):\n",
    "\n",
    "    test_df = pd.read_csv('test_simplified.csv')\n",
    "\n",
    "    # Get names of all train csv files with the pattern match below\n",
    "    fnames = glob('train_simplified/*.csv')\n",
    "\n",
    "    rows = recordsPerTrainClass\n",
    "    # Get n rows from all the csv files and append them into one dataframe\n",
    "    train_df = pd.DataFrame(columns=pd.read_csv(fnames[0], nrows=1).columns)\n",
    "    for name in fnames:\n",
    "        if skiprows == 0:\n",
    "            data = pd.read_csv(name, nrows=recordsPerTrainClass)\n",
    "        else:\n",
    "            data = pd.read_csv(name, nrows=recordsPerTrainClass, skiprows=range(1,skiprows))\n",
    "        train_df = train_df.append(data)\n",
    "\n",
    "#     print(train_df.shape)\n",
    "        \n",
    "    train_df = train_df.reset_index().drop('index', axis=1)\n",
    "    # Get only those which were correctly recognized\n",
    "    train_df = train_df[train_df['recognized'] == True]\n",
    "    \n",
    "    # Convert the drawing column to matrix\n",
    "#     train_df['drawing'] = train_df['drawing'].apply(ast.literal_eval)\n",
    "#     test_df['drawing'] = test_df['drawing'].apply(ast.literal_eval)\n",
    "    train_df['drawing'] = train_df['drawing'].apply(eval)\n",
    "    test_df['drawing'] = test_df['drawing'].apply(eval)\n",
    "\n",
    "    # Convert drawing to images\n",
    "    train_df['img'] = train_df['drawing'].apply(lambda x: draw_it(x, output_height, output_width))\n",
    "    test_df['img'] = test_df['drawing'].apply(lambda x: draw_it(x, output_height, output_width))\n",
    "    # train_df['img'] = train_df[['drawing']].apply(lambda x: draw_it(x['drawing'], output_height, output_width), axis=1)\n",
    "    # test_df['img'] = test_df[['drawing']].apply(lambda x: draw_it(x['drawing'], output_height, output_width), axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "def showSampleImgs():\n",
    "    n_samp = 3\n",
    "    train_df_sample = train_df.sample(n_samp)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    for i in range(n_samp):\n",
    "        draw = train_df_sample.iloc[i]['drawing']\n",
    "        label = train_df_sample.iloc[i]['word']\n",
    "        plt.subplot(n_samp,1,i+1)\n",
    "        for stroke in draw:\n",
    "            plt.plot(stroke[0], stroke[1], marker='.', color='black')\n",
    "            plt.title(label)\n",
    "            plt.axis('off')\n",
    "    plt.show()    \n",
    "    \n",
    "# Convert drawings to images\n",
    "def draw_it(raw_strokes, output_height, output_width):\n",
    "    image = Image.new(\"P\", (255,255)\n",
    "#                       , color=1\n",
    "            )\n",
    "    image_draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0])-1):\n",
    "\n",
    "            image_draw.line([stroke[0][i], \n",
    "                             stroke[1][i],\n",
    "                             stroke[0][i+1], \n",
    "                             stroke[1][i+1]],\n",
    "                            fill=255, width=6)\n",
    "    # Reduce image size\n",
    "    image = image.resize((output_height,output_width),Image.ANTIALIAS)\n",
    "    \n",
    "    return np.array(image)\n",
    "\n",
    "# Show an image from the dataframe\n",
    "def showImgFromDf(df, index):\n",
    "    # Show an image\n",
    "    plt.imshow(df.iloc[index]['img'],cmap='gray')\n",
    "    plt.title(df.iloc[index]['word'])\n",
    "    plt.show()\n",
    "    \n",
    "def CNN_dataPrep(train_df, img_height, img_width):\n",
    "    \n",
    "    num_classes = train_df['word'].nunique()\n",
    "    # Data Preprocessing\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.seed(111)\n",
    "    train_df = train_df.sample(train_df.shape[0])\n",
    "    \n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = img_height, img_width\n",
    "    input_shape = (img_rows, img_cols)\n",
    "    \n",
    "    # Reshape the array\n",
    "    imgArr = np.vstack(train_df['img'].values).flatten().reshape((train_df['img'].shape[0], img_rows, img_cols))\n",
    "    imgArr_test = np.vstack(test_df['img'].values).flatten().reshape((test_df['img'].shape[0], img_rows, img_cols))\n",
    "    \n",
    "    # In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [width][height][pixels] for TF.\n",
    "    # In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In this case, the pixel values are gray scale, the pixel dimension is set to 1.\n",
    "    imgArr = imgArr.reshape(imgArr.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    imgArr_test = imgArr_test.reshape(imgArr_test.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    \n",
    "    # Initialize the y_train\n",
    "    y_train = train_df['word']\n",
    "    \n",
    "    # Convert class labels from categorical to numerical\n",
    "    unique_classes_list = y_train.unique()\n",
    "    map_class_to_numeric = {k: v for v, k in enumerate(y_train.unique())}\n",
    "    map_numeric_to_class = {v: k for k, v in map_class_to_numeric.iteritems()}\n",
    "    y_train_numeric = y_train.apply(lambda x: map_class_to_numeric[x])\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train_one_hot = keras.utils.to_categorical(y_train_numeric, num_classes)\n",
    "    num_classes = y_train_one_hot.shape[1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(imgArr, y_train_one_hot, test_size=0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, imgArr_test, map_class_to_numeric, map_numeric_to_class\n",
    "    \n",
    "    \n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "\n",
    "def baseline_conv_model(num_filters, num_classes, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(num_filters, (5,5), input_shape=(img_rows,img_cols,1), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(num_filters*2, (5,5), input_shape=(img_rows,img_cols,1), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(num_filters*2, (3,3), input_shape=(img_rows,img_cols,1), activation='relu')) \n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.add(Dense(units=num_classes))\n",
    "#     model.add(Activation('softmax'))\n",
    "    model.add(Activation(tf.nn.softmax))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', top_3_accuracy, 'categorical_crossentropy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 8s, sys: 1min 48s, total: 14min 56s\n",
      "Wall time: 17min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "recordsPerTrainClass = 5000\n",
    "skiprows = 0\n",
    "img_height = img_width = 64\n",
    "\n",
    "# Get data and transform drawing into image\n",
    "train_df, test_df = generateTrainData(img_height, img_width, recordsPerTrainClass, skiprows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.2 s, sys: 1min 51s, total: 2min 31s\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Transform data for CNN\n",
    "X_train, X_test, y_train, y_test, imgArr_test, map_class_to_numeric, map_numeric_to_class = CNN_dataPrep(train_df, img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = pd.read_csv('test_simplified.csv', nrows=10)\n",
    "# p2 = pd.read_csv('test_simplified.csv', nrows=10)\n",
    "# p1 = pd.read_csv('test_simplified.csv', nrows=10, skiprows=range(1,20))\n",
    "# p2 = pd.read_csv('test_simplified.csv', nrows=10, skiprows=20)\n",
    "# p1.append(p2)\n",
    "# p1\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 8)         208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 16)        3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1936)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1937000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 340)               170340    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 340)               0         \n",
      "=================================================================\n",
      "Total params: 2,613,584\n",
      "Trainable params: 2,613,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1248341 samples, validate on 312086 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-b963075253ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           callbacks = callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_filters = 8\n",
    "num_classes = train_df['word'].nunique()\n",
    "continueTrain = True\n",
    "if continueTrain == False:\n",
    "    model = baseline_conv_model(num_filters, num_classes, img_height, img_width)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"Saved_Models/weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "hist = model.fit(X_train, \n",
    "          y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=2, \n",
    "          batch_size=5000, \n",
    "          verbose=2,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Error: 44.04%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112199/112199 [00:22<00:00, 4887.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "preds = model.predict(imgArr_test)\n",
    "outputDf = test_df.copy()[['key_id']]\n",
    "outputDf['word'] = ''\n",
    "map_numeric_to_class_space_normal = map_numeric_to_class.copy()\n",
    "for key in map_numeric_to_class_space_normal:\n",
    "    map_numeric_to_class_space_normal[key] = (map_numeric_to_class_space_normal[key].replace(\" \", \"_\"))\n",
    "\n",
    "for i in tqdm(range(preds.shape[0])):\n",
    "    outputDf['word'].at[i] = ' '.join(([map_numeric_to_class_space_normal[predClass] for predClass in [tup[1] for tup in sorted(zip(preds[i], range(340)), reverse=True)[:3]]]))\n",
    "\n",
    "# Create csv\n",
    "outputDf.to_csv('initial_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open((\"Saved_Models/model_\" + str(datetime.today())[:16].replace(\" \", \"_\").replace(\":\",\"-\") + \".json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights((\"Saved_Models/model_\" + str(datetime.today())[:16].replace(\" \", \"_\").replace(\":\",\"-\") + \".h5\"))\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load the model back\n",
    "# load json and create model\n",
    "json_file = open('Saved_Models/model_2018-11-11_17-15.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Saved_Models/model_2018-11-11_17-15.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', top_3_accuracy, 'categorical_crossentropy'])\n",
    "# score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
